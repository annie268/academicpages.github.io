<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/jpg" href="images/profile_old.png">
  <title>Annie S. Chen</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Annie S. Chen</name>
              </p>
              <p>Hi! I am a third-year computer science PhD student at Stanford University advised by Prof. <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> and affiliated with the <a href="https://ai.stanford.edu/">Stanford Artificial Intelligence Laboratory (SAIL)</a>. 
                My research goal is to create robust and adaptable machine learning systems that are prepared for distribution changes and efficiently respond to new information. 
                <!-- The focus of my work is on pushing the boundaries of robustness and adaptability in deep learning models in supervised settings and bringing these advances to build more intelligent sequential decision-making agents, resulting in more reliable systems for deployment.
                I am particularly interested in developing robust and adaptable models that respond efficiently and effectively to new information and distribution changes.  -->
                My work spans both supervised learning and reinforcement learning settings, with a focus on creating models that can generalize or adapt to changing environments.
                <!-- I am excited by problems involving interesting distributional shifts in various settings and work to develop models that can generalize or adapt to changing environments.  -->
                I am supported by an <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>.  
              </p>
              <p>Previously, I received a B.S. in math and M.S. in computer science, both also at Stanford. I have also been a research intern at <a href="https://research.google/teams/brain/">Google Brain</a>, where I learned a lot working with <a href="http://www.peteflorence.com/">Pete Florence</a>. 
              </p>
              <p>I am originally from Boulder, Colorado, and outside of research, I enjoy spending time outdoors, playing tennis, and learning to play the guitar. I care about creating an inclusive research culture and co-organize the <a href="https://stanfordcsmentoring.com/">Stanford CS Undergraduate Mentoring Program</a>, which matches undergraduate students with graduate student mentors and aims to increase the participation of underrepresented minorities in computer science research.  
              </p>
              <p>Please feel free to reach out about research or any advice I can help with!
              </p>
              <p  align="center">[<a href="mailto:asc8@cs.stanford.edu">Email</a>]
                [<a href="data/2023_10_CV.pdf">CV</a>]
                 [<a href="https://scholar.google.com/citations?user=bslhbWgAAAAJ&hl=en&oi=ao">Google Scholar</a>]
                 [<a href="https://twitter.com/_anniechen_">Twitter</a>]
                 [<a href="https://www.linkedin.com/in/annie-s-chen/">LinkedIn</a>]
                 [<a href="https://github.com/anniesch">GitHub</a>]
              </p>
            </td>
            <td width="10%">
              <img src="images/profile.jpg"
              width="160" 
              height="240">
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <!-- <p>I would one day like to have a robot that lives in my house and does all my chores for me. To that end, I study the intersection of planning and learning, especially in object-oriented and relational settings. My research uses techniques from classical planning, task and motion planning, program synthesis, reinforcement learning, and machine learning and sometimes takes inspiration from cognitive science.
              </p> -->
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/roam.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="papers/roam.pdf">
                <papertitle>Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen*</strong>, Govind Chada*, Laura Smith, Archit Sharma, Zipeng Fu, Sergey Levine, Chelsea Finn
              <br>
              <em>In submission</em>, 2023
              <br>
              [<a href="papers/roam.pdf">PDF</a>]
              [<a href="https://anniesch.github.io/adapt-on-the-go">Website</a>]
              <br>
              We propose Robust Autonomous Modulation (ROAM), a simple framework for efficiently leveraging pre-trained behaviors to adapt to changing situations at deployment time.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/cosmos_v2.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2306.11120">
                <papertitle>Confidence-Based Model Selection: When to Take Shortcuts for Subpopulation Shifts</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen</strong>, Yoonho Lee, Amrith Setlur, Sergey Levine, Chelsea Finn
              <br>
              <em>ArXiv Preprint</em>, 2023
              <br>
              [<a href="https://arxiv.org/pdf/2306.11120.pdf">PDF</a>]
              <br>
              We propose COnfidence-baSed MOdel Selection (COSMOS), where we adaptively choose among models with different strengths to achieve high performance on both majority and minority subpopulations. COSMOS does not require any target labels or group annotations and can even be used for hyperparameter tuning.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/voltron.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2302.12766">
                <papertitle>Language-Driven Representation Learning for Robotics</papertitle>
            </a>
              <br>
              Siddharth Karamcheti, Suraj Nair, <strong>Annie S. Chen</strong>, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, Percy Liang
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2023 <font color="red"><strong>(Best Paper Finalist)</strong></font>
              <br>
              [<a href="https://arxiv.org/pdf/2302.12766.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/voltron-robotics">Website</a>]
              [<a href="https://github.com/siddk/voltron-robotics">Code</a>]
              <br>
              We propose Voltron, which uses language to learn better visual representations for a diverse range of robotics problems by trading off conditioning and generation.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/Pro2.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2302.05441">
                <papertitle>Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen*</strong>, Yoonho Lee*, Amrith Setlur, Sergey Levine, Chelsea Finn
              <br>
              <em>ICLR TrustML-(un)Limited Workshop</em>, 2023 <font color="red"><strong>(Oral)</strong></font>
              <br>
              [<a href="https://arxiv.org/pdf/2302.05441.pdf">PDF</a>]
              <br>
              We propose Project and Probe (Pro^2), a lightweight + data-efficient approach for domain adaptation. Pro^2 first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro^2 then learns a linear classifier on top of these projected features using a small target dataset.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/surgical.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2210.11466">
                <papertitle>Surgical Fine-Tuning Improves Adaptation to Distribution Shifts</papertitle>
            </a>
              <br>
              Yoonho Lee*, <strong>Annie S. Chen*</strong>,  Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2023 
              <br>
              [<a href="https://arxiv.org/pdf/2210.11466.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/surgical-finetuning">Code</a>]
              <br>
              We show that selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms fine-tuning all layers. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/teaser_qwale.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2210.08863">
                <papertitle>You Only Live Once: Single-Life Reinforcement Learning</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen</strong>, Archit Sharma, Sergey Levine, Chelsea Finn
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2022 
              <br>
              [<a href="https://arxiv.org/pdf/2210.08863.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/single-life-rl">Code</a>]
              <br>
              Agents operating in the real world must often contend with novel situations that differ from their prior experience. In these situations, the agent only has one trial to complete the given task and must adapt on-the-fly to novelty without human interventions. To model such settings more formally, we study single-life reinforcement learning (SLRL) where given prior data, an agent must complete a task in a single trial in a domain with a novel distribution shift without any human interventions or supervision.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/pullfig_v1.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2103.16817">
                <papertitle>Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human Videos</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen</strong>, Suraj Nair, Chelsea Finn
              <br>
              <em>Robotics Science and Systems (RSS)</em>, 2021
              <br>
              [<a href="https://arxiv.org/pdf/2103.16817.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/dvd-human-videos">Website</a>]
              [<a href="https://github.com/anniesch/dvd">Code</a>]
              <br>
              We propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task. These reward functions can generalize to unseen environments and tasks by learning from a small amount of robot data and a large, diverse dataset of in-the-wild human videos.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/jtt.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2107.09044">
                <papertitle>Just Train Twice: Improving Group Robustness without Training Group Information</papertitle>
            </a>
              <br>
              Evan Z. Liu*, Behzad Haghgoo*, <strong>Annie S. Chen*</strong>,  Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, Chelsea Finn
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2021 <font color="red"><strong>(Long Talk)</strong></font>
              <br>
              [<a href="https://arxiv.org/pdf/2107.09044.pdf">PDF</a>]
              [<a href="https://github.com/anniesch/jtt">Code</a>]
              <br>
              A simple method that improves worst-group classification performance on datasets with spurious correlations without requiring training group annotations. JTT first detects informative training examples, which are often minority examples, by training an initial ERM classifier and extracting the misclassified examples. It then trains a final classifier by upsampling the selected examples.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/teaser_bee_4.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2010.11917">
                <papertitle>Batch Exploration with Examples for Scalable Robotic Reinforcement Learning</papertitle>
            </a>
              <br>
              <strong>Annie S. Chen*</strong>,  Hyunji Nam*, Suraj Nair*, Chelsea Finn
              <br>
              <em>Robotics and Automation Letters (RA-L)</em>, 2021 
              <br>
              [<a href="https://arxiv.org/pdf/2010.11917.pdf">PDF</a>]
              [<a href="https://sites.google.com/view/batch-exploration">Website</a>]
              [<a href="https://github.com/stanford-iris-lab/batch-exploration">Code</a>]
              <br>
              We propose a framework for leveraging weak human supervision to enable better robotic exploration for scalable data collection. Under this framework, the robot autonomously collects high quality data with a few minutes of human supervision, providing better data for downstream offline RL.
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/foundation_models.png' width="160" vspace="20"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2108.07258">
                <papertitle>On the Opportunities and Risks of Foundation Models</papertitle>
            </a>
              <br>
                Rishi Bommasani, ..., <strong>Annie S. Chen*</strong>, ..., Percy Liang
              <br>
              <em>Report by the Center for Research on Foundation Models (CRFM)</em>, 2021 
              <br>
              [<a href="https://arxiv.org/pdf/2108.07258.pdf">PDF</a>]
              <br>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <!-- <div class="one">
                <div class="two"><img src='images/teaser_bee_4.pdf' width="160" vspace="20"></div>
              </div> -->
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/1810.02425">
                <papertitle>Limit Theorems for Descents in Permutations and Arithmetic Progressions in Z/pZ</papertitle>
            </a>
              <br>
              Bryce Cai, <strong>Annie S. Chen</strong>, Ben Heller, Eyob Tsegaye
              <br>
              <em>Joint Mathematics Meetings (JMM) Undergraduate Poster Session</em>, 2019 <font color="red"><strong>(Outstanding Poster Presentation)</strong></font>
              <br>
              [<a href="https://arxiv.org/pdf/1810.02425.pdf">PDF</a>]
              <br>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <!-- <div class="one">
                <div class="two"><img src='images/teaser_bee_4.pdf' width="160" vspace="20"></div>
              </div> -->
            </td>
            <td valign="middle" width="75%">
            <a href="https://nyjm.albany.edu/j/2017/23-45v.pdf">
                <papertitle>Index Divisibility in Dynamical Sequences and Cyclic Orbits Modulo p</papertitle>
            </a>
              <br>
                <strong>Annie S. Chen</strong>, T. Alden Gassert, Katherine E. Stange
              <br>
              <em>New York Journal of Mathematics (NYJM)</em>, 2017
              <br>
              [<a href="https://nyjm.albany.edu/j/2017/23-45v.pdf">PDF</a>]
              <br>
            </td>
          </tr>

          


          <!-- <tr>
            <td width="25%">
              <div class="one">
                <div class="two"><img src='images/schemanet_image.png' width="160"></div>
              </div>
            </td>
            <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/1706.04317">
                <papertitle>Schema networks: zero-shot transfer with a generative causal model of intuitive physics</papertitle>
            </a>
              <br>
              Ken Kansky, <strong>Tom Silver</strong>, David A. Mely, Mohamed Eldawy, Miguel Lazaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, Dileep George 
              <br>
              <em>ICML</em>, 2017 <font color="red"><strong>(Spotlight Talk)</strong></font>
              <br>
              [<a href="bib/schemanetworks.bib">BibTeX</a>]
              [<a href="https://arxiv.org/pdf/1706.04317.pdf">PDF</a>]
              [<a href="https://www.vicarious.com/2017/08/07/general-game-playing-with-schema-networks/">Blog Post</a>] [<a href="https://www.youtube.com/watch?v=QHcAlAprFxA">Video</a>]
              [Press Coverage: 
               <a href="https://techcrunch.com/2017/07/25/vicarious-gets-another-50-million-to-expand-its-research-team-and-build-smarter-robots/">TechCrunch</a>, 
               <a href="https://www.wired.com/story/vicarious-schema-networks-artificial-intelligence-atari-demo/">Wired</a>,
               <a href="https://www.sciencemag.org/news/2018/05/how-researchers-are-teaching-ai-learn-child">Science</a>]
              <p></p>
              <p>We introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals.</p>
            </td>
          </tr> -->

        </td>
    </tr>
  </table>

  <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
      <tbody><tr>
        <td>
          <br>
          <p align="center">
            <font size="2">
              Website template from <a href="https://github.com/jonbarron/jonbarron_website">here.</a>
            </font>
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>
